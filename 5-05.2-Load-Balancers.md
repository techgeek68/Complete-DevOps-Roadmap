# Configuring Load Balancer Using Nginx

This guide covers the fundamentals, architecture, configuration, and advanced options for setting up Nginx as a load balancer in DevOps environments. It now includes cloud-native, dynamic, and global enhancements for scalability, reliability, and automation.

---

## What is a Load Balancer?

A **Load Balancer** is a software or hardware component that distributes incoming network traffic (HTTP/S, TCP, UDP) across multiple backend servers in a pool/farm. It ensures:
- **High Availability & Reliability:** No single point of failure; routes to healthy servers if one fails.
- **Scalability:** Easily add servers to handle more traffic.
- **Performance:** Balances load to avoid overloading, improving response times.
- **Redundancy & Maintenance:** Redirects traffic during updates/maintenance.
- **Health Checks:** Stops directing traffic to failed or unresponsive servers.

**Common Uses:**
- Websites & Applications (HTTP/HTTPS)
- APIs (REST, GraphQL, etc.)
- Databases (typically for read replicas)
- Network Services (DNS, FTP, SMTP, VPN)
- Cloud/Microservices (internal/external routing)
- Handling Traffic Spikes (flash sales, viral content)
- SSL/TLS Offloading (encryption at the load balancer)

---

## SELinux (Security Enhanced Linux)

SELinux provides mandatory access control (MAC) for fine-grained system security.
- **Enforcing:** SELinux is active.
- **Permissive:** Logs but does not block.
- **Disabled:** No enforcement.

**Key Commands:**
```bash
getenforce                    # Check SELinux status
sudo cat /var/log/audit/audit.log | grep nginx  # View Nginx-related logs
getsebool -a                  # List all SELinux booleans
getsebool -a | grep network   # List network-related booleans
setsebool -P httpd_can_network_connect=1  # Allow httpd/nginx network connections
getsebool -a | grep httpd_can_network_connect
```

---

## Prerequisites and Architecture

- **Three VMs:**  
  1. Load Balancer (`192.168.64.5`)
  2. Node 1 (`192.168.64.6`)
  3. Node 2 (`192.168.64.7`)
- **Shared Storage** (e.g., SAN/NAS/SCSI) for static assets (optional).

**Set Hostnames/IPs:**
```bash
hostnamectl set-hostname loadbalancer   # On Load Balancer VM
hostnamectl set-hostname node1          # On Node 1 VM
hostnamectl set-hostname node2          # On Node 2 VM
```

---

## Step 1: Install and Start Nginx (Load Balancer)

```bash
sudo yum install epel-release -y
sudo yum install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx
```

---

## Step 2: Open Firewall Ports

```bash
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload
```

---

## Step 3: Configure Nginx Load Balancer

**Create configuration file `/etc/nginx/conf.d/loadbalance.conf`:**

```nginx
upstream backend_servers {
    server 192.168.64.6;
    server 192.168.64.7;
}

server {
    listen 80;
    server_name 192.168.64.5;
    location / {
        proxy_pass http://backend_servers;
    }
}
```
- **Default method:** Round-robin.

**Check syntax and restart:**
```bash
sudo nginx -t
sudo systemctl restart nginx
```

---

## Step 4: Configure Backend Servers (Node 1 & Node 2)

**Install Nginx:**
```bash
sudo yum install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx
```

**Open firewall:**
```bash
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --reload
```

**Create index page:**
```bash
cd /usr/share/nginx/html
sudo mkdir devops
cd devops
sudo vim index.html
# <h1> Testing Load Balancer from node 1 </h1>   # On Node 1
# <h1> Testing Load Balancer from node 2 </h1>   # On Node 2
```

**Create server block config `/etc/nginx/conf.d/node1.conf` or `/etc/nginx/conf.d/node2.conf`:**
```nginx
server {
    listen 80;
    server_name <node_ip>;
    root /usr/share/nginx/html/devops;
    index index.html;
}
```
**Set root in `/etc/nginx/nginx.conf` as needed:**
```nginx
root /usr/share/nginx/html/devops;
```

**Check syntax and restart:**
```bash
sudo nginx -t
sudo systemctl restart nginx
```

---

## Step 5: Load Balancing Methods

**Round Robin (default):**  
Requests are distributed sequentially.
```nginx
upstream backend {
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**Least Connections:**  
Traffic goes to server with fewest active connections.
```nginx
upstream backend {
    least_conn;
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**IP Hash:**  
Session persistence based on client IP.
```nginx
upstream backend {
    ip_hash;
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**Health Checks:**  
Mark servers down after failures:
```nginx
server 192.168.1.10:80 max_fails=3 fail_timeout=30s;
```
- `max_fails`: Maximum failed attempts before marking as down.
- `fail_timeout`: Period server is considered down.

**Backup Server Example:**
```nginx
server 192.168.1.12:80 backup;
```

---

## Step 6: Full Example Configuration

```nginx
# /etc/nginx/conf.d/loadbalancer.conf
upstream backend {
    least_conn;
    server 192.168.1.10:80 max_fails=3 fail_timeout=30s;
    server 192.168.1.11:80;
    server 192.168.1.12:80 backup;
}

server {
    listen 80;
    server_name example.com;
    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $remote_addr;
    }
}
```

---

## Step 7: Enterprise & Cloud-Native Enhancements

### A. Dynamic Backend Registration (Service Discovery)
- Use tools such as **Consul**, **etcd**, or **Kubernetes Endpoints** to dynamically register/remove backend nodes.
- In Kubernetes, use an **Ingress Controller** (e.g., Nginx Ingress) to automatically watch for service endpoints and update backend pools.
- Example:  
  ```
  upstream backend {
      server consul-discovered-node-1:80;
      server consul-discovered-node-2:80;
  }
  ```
- Automate backend updates via API, templates, or orchestration.

### B. Automated Blue/Green & Canary Deployments
- Use deployment strategies to minimize risk and downtime.
- **Blue/Green:** Run two production environments (blue and green); switch traffic after validation.
- **Canary:** Gradually shift a percentage of traffic to new version, monitor, then increase.
- In Kubernetes, use labels/selectors or deployment tools (Argo Rollouts, Flagger).
- Example:  
  ```
  upstream backend {
      server old-version:80 weight=90;
      server new-version:80 weight=10;
  }
  ```
- Integrate with CI/CD for automation.

### C. Integration with Cloud-Native Load Balancers
- **AWS:** Use **Elastic Load Balancer (ELB/ALB/NLB)** for managed, scalable LB.
- **Azure:** Use **Azure Load Balancer** or **Application Gateway**.
- **GCP:** Use **Cloud Load Balancing**.
- Combine Nginx with cloud LB for hybrid architectures or full cloud-native.
- Map Nginx Ingress Service to cloud LB with annotations in Kubernetes.

### D. Auto-Scaling with Health-Aware Backend Registration
- Implement auto-scaling groups for backend servers.
- Load balancer should dynamically register new healthy instances and remove unhealthy ones.
- Use health checks and instance lifecycle hooks.
- In Kubernetes, Horizontal Pod Autoscaler (HPA) adjusts backend pods automatically.

### E. Distributed Tracing/APM for Request Flow
- Integrate distributed tracing tools (OpenTelemetry, Jaeger, Zipkin, Datadog APM) to track requests across load balancer and backend.
- Use Nginx tracing modules or log exporters to propagate trace headers.
- Example:
  ```
  proxy_set_header traceparent $traceparent;
  ```
- Visualize request flow and latency in APM dashboards.

### F. Advanced Traffic Shaping & Rate Limiting
- Use Nginx modules (`limit_req`, `limit_conn`) for rate limiting and traffic shaping.
- Example:
  ```nginx
  limit_req zone=req_limit_per_ip burst=10 nodelay;
  limit_conn zone=addr 10;
  ```
- Combine with API gateway or cloud WAF for advanced controls.

### G. Global Load Balancing (GeoDNS, Anycast)
- Deploy GeoDNS to route users to nearest datacenter/load balancer.
- Use Anycast IP addresses for global failover and distribution.
- Integrate cloud DNS services (AWS Route 53, Azure Traffic Manager, Google Cloud DNS) for global routing.
- Example:
  - GeoDNS record for `app.example.com` points to regional Nginx LBs.
  - Each LB handles local traffic and failover.

---

## Step 8: Best Practices & Advanced Topics

- **Monitor Nginx and Backend Health:** Use Prometheus/Grafana exporters, check logs frequently.
- **Automate with IaC:** Use Ansible or Terraform to manage Nginx config and server deployment.
- **SSL/TLS Offloading:** Terminate HTTPS at the load balancer for security and performance.
- **Web Application Firewall (WAF):** Integrate ModSecurity or a cloud WAF with Nginx for enhanced security.
- **Containerize Load Balancer:** Deploy Nginx as a container or in Kubernetes with Ingress controllers for dynamic scaling.
- **CI/CD Integration:** Automate config updates and reloads via pipelines.
- **SELinux:** Always check and set required booleans for Nginx networking.

---

## References

- [Nginx Load Balancing Docs](https://nginx.org/en/docs/http/load_balancing.html)
- [Consul Service Discovery](https://www.consul.io/docs/discovery)
- [Kubernetes Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
- [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)
- [AWS Elastic Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/)
- [Azure Load Balancer](https://learn.microsoft.com/en-us/azure/load-balancer/)
- [GCP Load Balancing](https://cloud.google.com/load-balancing/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Jaeger Tracing](https://www.jaegertracing.io/)
- [Nginx Rate Limiting](https://nginx.org/en/docs/http/ngx_http_limit_req_module.html)
- [GeoDNS with Route 53](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html)
- [Anycast networking](https://en.wikipedia.org/wiki/Anycast)
- [Prometheus Nginx Exporter](https://github.com/nginxinc/nginx-prometheus-exporter)
- [ModSecurity WAF](https://modsecurity.org/)

---

_Best Practice:_  
Automate deployment and configuration, monitor all layers, use modern load balancing algorithms, secure with SELinux/WAF/SSL, integrate dynamic cloud-native features (service discovery, scaling, tracing, traffic shaping, global routing), and containerize for scale and reliability.

---
