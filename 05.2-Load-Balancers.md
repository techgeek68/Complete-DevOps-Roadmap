## Load Balancer

This guide covers the fundamentals, architecture, configuration, and advanced options for setting up Nginx as a load balancer in DevOps environments.


---


**What is a Load Balancer?**

A Load Balancer is a software or hardware that distributes incoming network traffic (HTTP/S, TCP, UDP) across multiple backend servers in a pool/farm. 

- Its primary goal is to prevent any single server from becoming a bottleneck.
- Uses:
  - Websites & Web Applications (HTTP/HTTPS)
  - API endpoints (REST, GraphQL, etc.)
  - Database read replicas
  - Network Services (DNS, FTP, SMTP, VPN)
  - Microservices architectures (internal/external routing)
  - Traffic spike management (flash sales, viral content)
  - SSL/TLS Offloading (encryption at the load balancer)
- Advantages :
  - Improved Reliability:
      - Ensures applications remain available.
  - Enhanced Performance:
      - Reduces latency and server load.
  - Increased Scalability:
      - Allows easy addition of servers to handle growth.


| Function | Description |
| :--- | :--- |
| **Traffic Distribution** | Efficiently directs incoming client requests to available servers. |
| **High Availability** | Automatically detects server failures and redirects traffic to healthy servers, preventing downtime. |
| **Scalability** | Enables seamless addition of servers to the backend pool without impacting users. |
| **Performance Optimization** | Ensures requests are processed by the most capable or available server, reducing load and latency. |


---


**Load Balancing Algorithms**
  - The algorithm determines how the load balancer distributes traffic.

| Algorithm | Principle | Best For |
| :--- | :--- | :--- |
| **Round Robin** | Distributes requests sequentially to each server in a loop. | Simple, uniform server performance. |
| **Least Connections** | Directs traffic to the server with the fewest active connections. | Long-lived connections (e.g., databases, live streams). |
| **Least Response Time** | Routes to the server with the fastest response time *and* fewest connections. | Performance-critical applications require low latency. |
| **IP Hash** | Uses the client's IP address to determine which server to use, ensuring the same client always reaches the same server. | Session persistence (e.g., shopping carts, user logins). |


**Load Balancer Request Flow**
* A typical request cycle follows these steps:
  * A user sends a request (e.g., to visit a website).
  * The request is received by the Load Balancer.
  * The load balancer uses a pre-configured Algorithm to select the optimal backend server.
  * The load balancer forwards the request to the chosen server.
  * The server processes the request and sends the response back to the user (often via the load balancer).


---


**Types of Load Balancer**: Categorized by their form factor and deployment
*   **Hardware Load Balancers**
    *   Dedicated physical devices.
    *   High performance, can handle massive traffic volumes.
    *   Expensive, less flexible.

*   **Software Load Balancers**
    *   Applications running on standard servers or VMs.
    *   More flexible, cost-effective.
    *   Requires management of the underlying OS.

*   **Cloud Load Balancers**
    *   Managed services from cloud providers (e.g., AWS ELB, Azure Load Balancer).
    *   Automatically handles scaling, maintenance, and is highly available.
    *   Vendor-specific.

---


**SELinux (Security Enhanced Linux)**

SELinux provides mandatory access control (MAC) for fine-grained system security.
  - **Enforcing:** SELinux is active.
  - **Permissive:** Logs but does not block.
  - **Disabled:** No enforcement.

**Commands:**
```bash
getenforce                                        # Check SELinux status

sudo cat /var/log/audit/audit.log | grep nginx    # View Nginx-related logs

getsebool -a                                      # List all SELinux booleans

getsebool -a | grep network                       # List network-related booleans

setsebool -P httpd_can_network_connect=1          # Allow httpd/nginx network connections
getsebool -a | grep httpd_can_network_connect     # Verify the setting
```

---

**Prerequisites and Architecture**
  - To complete this lab, the following requirements are needed.
  - Shared Storage (e.g., SAN/NAS/SCSI) for static assets (optional).
  - Three VMs
    - Load Balancer
    - Node 1 
    - Node 2


<img width="961" height="374" alt="Screenshot 2025-11-10 at 6 25 26 AM" src="https://github.com/user-attachments/assets/c70b56ea-e4fe-491b-96da-8ff98ae96252" />



- Set Hostnames/IPs:
```bash
  hostnamectl set-hostname loadbalancer     # On Load Balancer VM
  hostnamectl set-hostname node1            # On Node 1 VM
  hostnamectl set-hostname node2            # On Node 2 VM
```

- Network setup
  - Load Balancer (IPv4: x.x.x.x)
  - Node 1 (IPv4: x.x.x.x)
  - Node 2 (IPv4: x.x.x.x)


![Screenshot 2025-11-09 at 12 21 29 PM](https://github.com/user-attachments/assets/6386ae2b-d188-4c9d-b7de-f83250e94ea1)


---


**Load Balancer Setup**

**Step 1: Install Nginx on Load Balancer**

```bash
# RHEL/CentOS/Fedora
sudo yum install epel-release -y
sudo yum install nginx -y

# Debian/Ubuntu
sudo apt update
sudo apt install nginx -y

# Start and enable Nginx
sudo systemctl start nginx
sudo systemctl enable nginx
sudo systemctl status nginx
```



**Step 2: Configure Firewall**

```bash
# Open HTTP and HTTPS ports
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload

# Alternative: Specific ports
sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload
```


**Step 3: Configure Load Balancing**

  - Create `sudo vim /etc/nginx/conf.d/loadbalancer.conf`

```nginx
upstream backend_servers {
    # Basic round-robin (default)
    server 10.10.0.7;
    server 10.10.0.8;
}

server {
    listen 80;
    server_name 10.10.0.6;
    
    access_log /var/log/nginx/lb_access.log;
    error_log /var/log/nginx/lb_error.log;
    
    location / {
        proxy_pass http://backend_servers;
        
        # Essential proxy headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeout settings
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

  - Check syntax and restart:
  ```bash
  sudo nginx -t
  
  sudo systemctl restart nginx
  ```


**Step 4: Configure Backend Servers**

- On both Node
```bash
# Install Nginx
sudo yum install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx

# Configure firewall
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --reload
```

- Create an unique index page on both Node.
  
```bash
sudo mkdir -p /usr/share/nginx/html/devops
```

  * Change the Document root location:
    * sudo vim /etc/nginx/nginx.conf
      * `root         /usr/share/nginx/html/devops;`

- On Node 1: `sudo vim /usr/share/nginx/html/devops/index.html`
```html
<!DOCTYPE html>
<html>
<head>
  <title>Backend Server</title>
</head>
<body>
  <h1>Backend Server</h1>
  <h2>This is a load balancer test page</h2>
  <p>Served by node1.</p>
</body>
</html>
```

- On Node 2: `sudo vim /usr/share/nginx/html/devops/index.html`
```html
<!DOCTYPE html>
<html>
<head>
  <title>Backend Server</title>
</head>
<body>
  <h1>Backend Server</h1>
  <h2>This is a load balancer test page</h2>
  <p>Served by node2.</p>
</body>
</html>
```

**Create server block config `/etc/nginx/conf.d/node1.conf` or `/etc/nginx/conf.d/node2.conf`:**

`sudo vim /etc/nginx/conf.d/backend.conf`

-  On node1
```nginx
server {
        listen 80;
        server_name 10.10.0.7;

        root /usr/share/nginx/html/devops;
        index index.html;

        access_log /var/log/nginx/backend_access.log;
        error_log /var/log/nginx/backend_error.log;

        location / {
                try_files $uri $uri/ =404;
        }

        location /health {
                access_log off;
                default_type text/plain;
                return 200 "healthy\n";
        }
}
```

-  On node2:
```nginx
server {
        listen 80;
        server_name 10.10.0.8;

        root /usr/share/nginx/html/devops;
        index index.html;

        access_log /var/log/nginx/backend_access.log;
        error_log /var/log/nginx/backend_error.log;

        location / {
                try_files $uri $uri/ =404;
        }

        
        # Or
        # Common for simple static sites
        #location / {
              #try_files $uri $uri/ /index.html;
          #}
          

        location /health {
                access_log off;
                default_type text/plain;
                return 200 "healthy\n";
        }
}
```

- Check syntax
```bash
sudo nginx -t
```

- Check syntax
```bash
sudo systemctl restart nginx
```

- Open a browser on the host computer and visit http://10.10.0.6
  
<img width="800" height="257" alt="Screenshot 2025-11-10 at 8 52 47 AM" src="https://github.com/user-attachments/assets/be17825a-d91d-4fee-ae62-114ba04ca0c3" />

- Reload the page

<img width="802" height="255" alt="Screenshot 2025-11-10 at 8 52 56 AM" src="https://github.com/user-attachments/assets/0262ef86-e6e8-4cff-8ed8-618cf0d205a8" />


---

**Load Balancing Methods**

**A. Round Robin (default):**  
  - Distributes requests sequentially to each server in rotation.
```nginx
Syntax
upstream backend_name {
    server host:port;
    server host:port;
}

Example:
upstream backend {
    server 192.168.1.10;
    server 192.168.1.11;
    .
    .
}
```

**B. Least Connections:**  
- Routes traffic to the server with the fewest active connections.
```nginx
Syntax:
upstream backend_name {
    least_conn;
    server host:port;
}

Example:
upstream backend {
    least_conn;
    server 192.168.1.10;
    server 192.168.1.11;
    .
    .
}
```

**C. IP Hash (Session Persistence)**
  - Uses client IP to determine server selection, maintaining session affinity.
```nginx
Syntax:
upstream backend_name {
    ip_hash;
    server host:port;
}

Example:
upstream backend {
    ip_hash;
    server 192.168.1.10;
    server 192.168.1.11;
    .
    .
}
```

**D. Weighted Load Balancing**
  - Distributes traffic proportionally based on assigned weights to each server.
```nginx
Syntax:
upstream backend_name {
    server host:port weight=number;
    server host:port weight=number;
}

Example:
upstream backend {
    server 192.168.64.6:80 weight=3;        # 60% of traffic
    server 192.168.64.7:80 weight=2;        # 40% of traffic
}
```

**E. Generic Hash**
  - Distributes requests based on a specified key(hashed value) (e.g., URL) for consistent routing.
```nginx
Syntax:
upstream backend_name {
    hash key;
    server host:port;
}

Example:
upstream backend {
    hash $request_uri;
    server 192.168.1.10;
    server 192.168.1.11;
}
```

**F. Random with Two Choices**
  - Randomly selects servers, optionally with least connections optimization.
```nginx
Syntax:
upstream backend_name {
    random [two];
    server host:port;
}

Example:
upstream backend {
    random two;
    server 192.168.1.10;
    server 192.168.1.11;
}
```

---


**Health Checks and Failover Configuration Syntax**

```nginx
upstream backend_name {
    server host:port max_fails=number fail_timeout=time;
    server host:port max_fails=number fail_timeout=time;
    server host:port backup;
}
```

**Server Health Parameters:**
- `max_fails=number` - *Number of consecutive failures before considering server unhealthy*
- `fail_timeout=time` - *Time period server remains marked as failed (e.g., 30s)*
- `backup` - *Designates server as backup (only used when primary servers fail)*



**Enhanced Health Check Directives**

```nginx
location /path {
    proxy_pass http://upstream_name;
    
    # Failure conditions for trying next server
    proxy_next_upstream error_type [error_type...];
    
    # Retry limits
    proxy_next_upstream_tries number;
    proxy_next_upstream_timeout time;
    
    # Essential headers for backend
    proxy_set_header Header_Name $variable;
}
```


**Health Check Parameters:**
- `proxy_next_upstream` - *Conditions to attempt next server:*
  - `error` - *Connection/communication error*
  - `timeout` - *Timeout occurred*
  - `http_500` - *Backend returned HTTP 500*
  - `http_502` - *Backend returned HTTP 502*
  - `http_503` - *Backend returned HTTP 503*
  - `http_504` - *Backend returned HTTP 504*

- `proxy_next_upstream_tries` - *Maximum number of attempts to different servers*
- `proxy_next_upstream_timeout` - *Time limit for next upstream attempts*

**Essential Headers:**
- `Host $host` - *Preserves original request host*
- `X-Real-IP $remote_addr` - *Forwards client IP to backend*
- `X-Forwarded-For $proxy_add_x_forwarded_for` - *Maintains client IP chain*




**Load Balancer Monitoring**

```nginx
location /monitoring_path {
    stub_status on;
    access_log on|off;
    allow network;
    deny all;
}
```

**Monitoring Directives:**
- `stub_status on` - *Enables Nginx status page*
- `access_log off` - *Disables logging for status endpoint*
- `allow/deny` - *Restricts access to monitoring page*

**Status Page Provides:**
- Active connections
- Request statistics
- Server status information
- Connection metrics

**Backup Server**
- Designates the server as a backup, only used when all primary servers are unavailable
- Remains inactive during normal operation
- Automatically activated when all primary servers fail health checks
- Returns to standby mode when primary servers recover
- Provides automatic failover capability
- Syntax
```nginx
server host:port backup;
```


**Example Configuration**

```nginx
# /etc/nginx/conf.d/loadbalancer.conf
upstream backend {
    least_conn;                                            # Use the least connections load balancing method
    server 192.168.1.10:80 max_fails=3 fail_timeout=30s;   # Primary server with health checks
    server 192.168.1.11:80;                                # Secondary primary server
    server 192.168.1.12:80 backup;                         # Backup server (active only when primaries fail)
}

server {
    listen 80;                                             # Listen on port 80 for HTTP traffic
    server_name example.com;                               # Domain name for this virtual host
    location / {
        proxy_pass http://backend;                         # Forward requests to backend upstream group
        proxy_set_header Host $host;                       # Preserve original host header
        proxy_set_header X-Forwarded-For $remote_addr;     # Forward client IP to backend
    }
}
```
 

---


**Enterprise & Cloud-Native Enhancements**

**A. Dynamic Backend Registration (Service Discovery)**
- Automatically update load balancer backends using service discovery tools instead of static configuration.
- Implementation Methods
  - Service Discovery Integration
    - Use Consul, etcd, or Kubernetes API for real-time node registration
    - Nginx Ingress Controller automatically watches Kubernetes endpoints

  - DNS-Based Discovery
```nginx
    upstream backend {
    server backend-service.internal.com resolve;  # DNS-based service discovery
    
    resolver 10.0.0.2 valid=30s;                 # DNS server with TTL
    resolver_timeout 5s;                         # DNS query timeout
}
```

  - Template-Based Automation
```nginx
    upstream backend {
    {% for server in service "web" %}            # Consul Template loop
    server {{ server.Address }}:{{ server.Port }};  # Dynamic server entries
    {% endfor %}
}
```
    
  - Advantages
    - Automatic scaling: New nodes automatically join the pool
    - Self-healing: Failed nodes automatically removed
    - Zero-downtime updates: Backend changes without config reloads
    - Cloud-native: Integrates with container orchestration platforms


**B. Automated Blue/Green & Canary Deployments**
- Advanced deployment strategies to minimize risk and enable zero-downtime releases.
- Deployment Methods:
  - Blue/Green:
    - Run two production environments (blue and green); switch traffic after validation.
    - Deploy new version alongside old → validate → switch all traffic
```nginx
upstream backend {
    server blue-environment:80;    # Current stable version (100% traffic)
    server green-environment:80;   # New version (0% traffic - ready for switch)
}
```
  - Canary:
    - Route a small percentage to the new version → monitor metrics → gradually increase
    - Gradually shift a percentage of traffic to the new version, monitor, then increase.
```nginx
upstream backend {
    server stable-version:80 weight=90;    # 90% traffic to stable
    server canary-version:80 weight=10;    # 10% traffic to new version
}
```
  - Session-Based Canary with Sticky Sessions
    - Automatic reversion if health checks fail
```nginx
upstream backend {
    hash $cookie_canary consistent;        # Session persistence for canary testing
    server stable-version:80;
    server canary-version:80;
}
```

  - Automated Rollback
    - Automatic reversion if health checks fail


- Implementation Tools:
  - Kubernetes:
      Labels, selectors, and service meshes

  - Orchestration:
      Argo Rollouts, Flagger for automated traffic shifting

  - CI/CD Integration:
      Automated promotion based on metrics and health checks

- Advantages
  - Zero-downtime deployments
  - Reduced risk with gradual exposure
  - Instant rollback capabilities
  - Real-time performance monitoring during releases 


**C. Integration with Cloud-Native Load Balancers**
- **AWS:**
  - Use Elastic Load Balancer (ELB/ALB/NLB) for managed, scalable LB.
- **Azure:**
  - Use Azure Load Balancer or Application Gateway.
- **GCP:**
  - Use Cloud Load Balancing.
- Combine Nginx with cloud LB for hybrid architectures or full cloud-native.
- Map Nginx Ingress Service to cloud LB with annotations in Kubernetes.


**D. Auto-Scaling with Health-Aware Backend Registration**
- Implement auto-scaling groups for backend servers.
- Load balancer should dynamically register new healthy instances and remove unhealthy ones.
- Use health checks and instance lifecycle hooks.
- In Kubernetes, Horizontal Pod Autoscaler (HPA) adjusts backend pods automatically.


**E. Distributed Tracing/Application Performance Monitoring (APM) for Request Flow**
- Integrate distributed tracing tools (OpenTelemetry, Jaeger, Zipkin, Datadog APM) to track requests across the load balancer and backend.
- Use Nginx tracing modules or log exporters to propagate trace headers.
- Example:
  ```
  proxy_set_header traceparent $traceparent;
  ```
- Visualize request flow and latency in APM dashboards.


**F. Advanced Traffic Shaping & Rate Limiting**

- Control and manage traffic flow using Nginx's built-in modules to prevent abuse, ensure fairness, and maintain service stability.

**1. Rate Limiting Configuration**

  - Request Rate Limiting:
```nginx
# Define rate limit zone (10 requests per second per IP)
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

# Define connection limit zone  
limit_conn_zone $binary_remote_addr zone=addr:10m;

server {
    location /api/ {
        limit_req zone=api burst=20 nodelay;    # Allow bursting, no delay
        limit_conn addr 10;                     # Max 10 concurrent connections
        proxy_pass http://backend;
    }
}
```

- Parameters Explained:
  - `zone=name:size` - *Shared memory zone for storing state*
  - `rate=requests` - *Maximum request rate (r/s=requests per second, r/m=per minute)*
  - `burst=number` - *Number of excessive requests to queue*
  - `nodelay` - *Process burst without delay up to burst size*
  - `limit_conn` - *Limit concurrent connections per key*


**2. Advanced Traffic Management**

  - Selective Rate Limiting:
```nginx
# Different limits for different endpoints
location /api/public/ {
    limit_req zone=public_api burst=50 nodelay;  # More lenient for public API
}

location /api/private/ {
    limit_req zone=private_api burst=10 nodelay; # Stricter for private API
}

location /static/ {
    # No rate limiting for static content
    proxy_pass http://backend;
}
```

  - Geographic or User-Based Limits:
```nginx
# Limit by geographic location or user type
geo $limited_region {
    default 0;
    192.168.1.0/24 1;  # Internal network - no limits
}

map $limited_region $limit_key {
    0 $binary_remote_addr;  # External users - limit by IP
    1 "";                   # Internal users - no limits
}

limit_req_zone $limit_key zone=geo_aware:10m rate=5r/s;
```


**3. Integration with Advanced Tools**

- API Gateway Features:
  - Combine with **Kong**, **Tyk**, or **AWS API Gateway** for advanced policies
  - Implement **JWT-based rate limiting**
  - **Quota management** per API key or user

- Cloud WAF Integration:
  - Use with **Cloudflare**, **AWS WAF**, or **Azure WAF**
  - **DDoS protection** with automatic traffic filtering
  - **Bot detection** and mitigation

**Monitoring and Analytics:**
```nginx
location /api/ {
    limit_req zone=api burst=20 nodelay;
    
    # Log rate limit events
    limit_req_status 429;
    access_log /var/log/nginx/rate_limits.log;
    
    proxy_pass http://backend;
}
```

**4. Benefits of Traffic Shaping**
  - Service Protection:
      Prevent resource exhaustion and DDoS attacks
  - Fair Usage:
      Ensure equitable resource distribution among users  
  - Quality of Service:
      Maintain performance during traffic spikes
  - Cost Control:
      Reduce infrastructure costs by eliminating abuse
  - Compliance:
      Meet API rate limiting requirements for developers


**G. Global Load Balancing (GeoDNS, Anycast)**
- Deploy GeoDNS to route users to the nearest datacenter/load balancer.
- Use Anycast IP addresses for global failover and distribution.
- Integrate cloud DNS services (AWS Route 53, Azure Traffic Manager, Google Cloud DNS) for global routing.
- Example:
  - GeoDNS record for `app.example.com` points to regional Nginx LBs.
  - Each LB handles local traffic and failover.


---


**Recommendations**

- **Monitor Nginx and Backend Health:**
    - Use Prometheus/Grafana exporters, check logs frequently.
      
- **Automate with IaC:**
    - Use Ansible or Terraform to manage Nginx config and server deployment.
    
- **SSL/TLS Offloading:**
    - Terminate HTTPS at the load balancer for security and performance.
      
- **Web Application Firewall (WAF):**
    - Integrate ModSecurity or a cloud WAF with Nginx for enhanced security.
      
- **Containerize Load Balancer:**
    - Deploy Nginx as a container or in Kubernetes with Ingress controllers for dynamic scaling.
      
- **CI/CD Integration:**
    - Automate config updates and reloads via pipelines.
      
- **SELinux:**
    - Always check and set the required booleans for Nginx networking.


---


**References**

- [Nginx Load Balancing Docs](https://nginx.org/en/docs/http/load_balancing.html)
- [Consul Service Discovery](https://www.consul.io/docs/discovery)
- [Kubernetes Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
- [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)
- [AWS Elastic Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/)
- [Azure Load Balancer](https://learn.microsoft.com/en-us/azure/load-balancer/)
- [GCP Load Balancing](https://cloud.google.com/load-balancing/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Jaeger Tracing](https://www.jaegertracing.io/)
- [Nginx Rate Limiting](https://nginx.org/en/docs/http/ngx_http_limit_req_module.html)
- [GeoDNS with Route 53](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html)
- [Anycast networking](https://en.wikipedia.org/wiki/Anycast)
- [Prometheus Nginx Exporter](https://github.com/nginxinc/nginx-prometheus-exporter)
- [ModSecurity WAF](https://modsecurity.org/)

---
