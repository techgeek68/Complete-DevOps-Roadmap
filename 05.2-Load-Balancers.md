## Load Balancer

This guide covers the fundamentals, architecture, configuration, and advanced options for setting up Nginx as a load balancer in DevOps environments.


---


**What is a Load Balancer?**

A Load Balancer is a software or hardware that distributes incoming network traffic (HTTP/S, TCP, UDP) across multiple backend servers in a pool/farm. 

- Its primary goal is to prevent any single server from becoming a bottleneck.
- Uses:
  - Websites & Web Applications (HTTP/HTTPS)
  - API endpoints (REST, GraphQL, etc.)
  - Database read replicas
  - Network Services (DNS, FTP, SMTP, VPN)
  - Microservices architectures (internal/external routing)
  - Traffic spike management (flash sales, viral content)
  - SSL/TLS Offloading (encryption at the load balancer)
- Advantages :
  - Improved Reliability:
      - Ensures applications remain available.
  - Enhanced Performance:
      - Reduces latency and server load.
  - Increased Scalability:
      - Allows easy addition of servers to handle growth.


| Function | Description |
| :--- | :--- |
| **Traffic Distribution** | Efficiently directs incoming client requests to available servers. |
| **High Availability** | Automatically detects server failures and redirects traffic to healthy servers, preventing downtime. |
| **Scalability** | Enables seamless addition of servers to the backend pool without impacting users. |
| **Performance Optimization** | Ensures requests are processed by the most capable or available server, reducing load and latency. |


**Load Balancing Algorithms**
  - The algorithm determines how the load balancer distributes traffic.

| Algorithm | Principle | Best For |
| :--- | :--- | :--- |
| **Round Robin** | Distributes requests sequentially to each server in a loop. | Simple, uniform server performance. |
| **Least Connections** | Directs traffic to the server with the fewest active connections. | Long-lived connections (e.g., databases, live streams). |
| **Least Response Time** | Routes to the server with the fastest response time *and* fewest connections. | Performance-critical applications require low latency. |
| **IP Hash** | Uses the client's IP address to determine which server to use, ensuring the same client always reaches the same server. | Session persistence (e.g., shopping carts, user logins). |


**Request Flow**
A typical request cycle follows these steps:
  * A user sends a request (e.g., to visit a website).
  * The request is received by the Load Balancer.
  * The load balancer uses a pre-configured Algorithm to select the optimal backend server.
  * The load balancer forwards the request to the chosen server.
  * The server processes the request and sends the response back to the user (often via the load balancer).


 **Types**
Categorized by their form factor and deployment:

*   **Hardware Load Balancers**
    *   Dedicated physical devices.
    *   High performance, can handle massive traffic volumes.
    *   Expensive, less flexible.

*   **Software Load Balancers**
    *   Applications running on standard servers or VMs.
    *   More flexible, cost-effective.
    *   Requires management of the underlying OS.

*   **Cloud Load Balancers**
    *   Managed services from cloud providers (e.g., AWS ELB, Azure Load Balancer).
    *   Automatically handles scaling, maintenance, and is highly available.
    *   Vendor-specific.

---


**SELinux (Security Enhanced Linux)**

SELinux provides mandatory access control (MAC) for fine-grained system security.
  - **Enforcing:** SELinux is active.
  - **Permissive:** Logs but does not block.
  - **Disabled:** No enforcement.

**Commands:**
```bash
getenforce                                        # Check SELinux status

sudo cat /var/log/audit/audit.log | grep nginx    # View Nginx-related logs

getsebool -a                                      # List all SELinux booleans

getsebool -a | grep network                       # List network-related booleans

setsebool -P httpd_can_network_connect=1          # Allow httpd/nginx network connections
getsebool -a | grep httpd_can_network_connect     # Verify the setting
```

---

**Prerequisites and Architecture**
  - To complete this lab, the following requirements are needed.
  - Shared Storage (e.g., SAN/NAS/SCSI) for static assets (optional).
  - Three VMs
    - Load Balancer
    - Node 1 
    - Node 2


<img width="961" height="374" alt="Screenshot 2025-11-10 at 6 25 26 AM" src="https://github.com/user-attachments/assets/c70b56ea-e4fe-491b-96da-8ff98ae96252" />



- Set Hostnames/IPs:
```bash
  hostnamectl set-hostname loadbalancer     # On Load Balancer VM
  hostnamectl set-hostname node1            # On Node 1 VM
  hostnamectl set-hostname node2            # On Node 2 VM
```

- Network setup
  - Load Balancer (IPv4: x.x.x.x)
  - Node 1 (IPv4: x.x.x.x)
  - Node 2 (IPv4: x.x.x.x)


![Screenshot 2025-11-09 at 12 21 29 PM](https://github.com/user-attachments/assets/6386ae2b-d188-4c9d-b7de-f83250e94ea1)


---


**Load Balancer Setup**

**Step 1: Install Nginx on Load Balancer**

```bash
# RHEL/CentOS/Fedora
sudo yum install epel-release -y
sudo yum install nginx -y

# Debian/Ubuntu
sudo apt update
sudo apt install nginx -y

# Start and enable Nginx
sudo systemctl start nginx
sudo systemctl enable nginx
sudo systemctl status nginx
```



**Step 2: Configure Firewall**

```bash
# Open HTTP and HTTPS ports
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload

# Alternative: Specific ports
sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --permanent --add-port=443/tcp
sudo firewall-cmd --reload
```


**Step 3: Configure Load Balancing**

  - Create `sudo vim /etc/nginx/conf.d/loadbalancer.conf`

```nginx
upstream backend_servers {
    # Basic round-robin (default)
    server 10.10.0.7;
    server 10.10.0.8;
}

server {
    listen 80;
    server_name 10.10.0.6;
    
    access_log /var/log/nginx/lb_access.log;
    error_log /var/log/nginx/lb_error.log;
    
    location / {
        proxy_pass http://backend_servers;
        
        # Essential proxy headers
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeout settings
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

  - Check syntax and restart:
  ```bash
  sudo nginx -t
  
  sudo systemctl restart nginx
  ```


**Step 4: Configure Backend Servers**

- On both Node
```bash
# Install Nginx
sudo yum install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx

# Configure firewall
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --reload
```

- Create an unique index page on both Node.
  
```bash
sudo mkdir -p /usr/share/nginx/html/devops
```

  * Change the Document root location:
    * sudo vim /etc/nginx/nginx.conf
      * `root         /usr/share/nginx/html/devops;`

- On Node 1: `sudo vim /usr/share/nginx/html/devops/index.html`
```html
<!DOCTYPE html>
<html>
<head>
  <title>Backend Server</title>
</head>
<body>
  <h1>Backend Server</h1>
  <h2>This is a load balancer test page</h2>
  <p>Served by node1.</p>
</body>
</html>
```

- On Node 2: `sudo vim /usr/share/nginx/html/devops/index.html`
```html
<!DOCTYPE html>
<html>
<head>
  <title>Backend Server</title>
</head>
<body>
  <h1>Backend Server</h1>
  <h2>This is a load balancer test page</h2>
  <p>Served by node2.</p>
</body>
</html>
```

**Create server block config `/etc/nginx/conf.d/node1.conf` or `/etc/nginx/conf.d/node2.conf`:**

`sudo vim /etc/nginx/conf.d/backend.conf`

```nginx
server {
    listen 80;
    server_name $(hostname -I | awk '{print $1}');
    
    root /usr/share/nginx/html/devops;
    index index.html;
    
    access_log /var/log/nginx/backend_access.log;
    error_log /var/log/nginx/backend_error.log;
    
    location / {
        try_files \$uri \$uri/ =404;
    }
    
    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

**Test and restart**
```bash
sudo nginx -t
sudo systemctl restart nginx


**Check syntax and restart:**
```bash
sudo nginx -t
sudo systemctl restart nginx
```



## Step 5: Load Balancing Methods

**Round Robin (default):**  
Requests are distributed sequentially.
```nginx
upstream backend {
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**Least Connections:**  
Traffic goes to server with fewest active connections.
```nginx
upstream backend {
    least_conn;
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**IP Hash:**  
Session persistence based on client IP.
```nginx
upstream backend {
    ip_hash;
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}
```

**Health Checks:**  
Mark servers down after failures:
```nginx
server 192.168.1.10:80 max_fails=3 fail_timeout=30s;
```
- `max_fails`: Maximum failed attempts before marking as down.
- `fail_timeout`: Period server is considered down.

**Backup Server Example:**
```nginx
server 192.168.1.12:80 backup;
```

---

## Step 6: Full Example Configuration

```nginx
# /etc/nginx/conf.d/loadbalancer.conf
upstream backend {
    least_conn;
    server 192.168.1.10:80 max_fails=3 fail_timeout=30s;
    server 192.168.1.11:80;
    server 192.168.1.12:80 backup;
}

server {
    listen 80;
    server_name example.com;
    location / {
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $remote_addr;
    }
}
```

---

## Step 7: Enterprise & Cloud-Native Enhancements

### A. Dynamic Backend Registration (Service Discovery)
- Use tools such as **Consul**, **etcd**, or **Kubernetes Endpoints** to dynamically register/remove backend nodes.
- In Kubernetes, use an **Ingress Controller** (e.g., Nginx Ingress) to automatically watch for service endpoints and update backend pools.
- Example:  
  ```
  upstream backend {
      server consul-discovered-node-1:80;
      server consul-discovered-node-2:80;
  }
  ```
- Automate backend updates via API, templates, or orchestration.

### B. Automated Blue/Green & Canary Deployments
- Use deployment strategies to minimize risk and downtime.
- **Blue/Green:** Run two production environments (blue and green); switch traffic after validation.
- **Canary:** Gradually shift a percentage of traffic to new version, monitor, then increase.
- In Kubernetes, use labels/selectors or deployment tools (Argo Rollouts, Flagger).
- Example:  
  ```
  upstream backend {
      server old-version:80 weight=90;
      server new-version:80 weight=10;
  }
  ```
- Integrate with CI/CD for automation.

### C. Integration with Cloud-Native Load Balancers
- **AWS:** Use **Elastic Load Balancer (ELB/ALB/NLB)** for managed, scalable LB.
- **Azure:** Use **Azure Load Balancer** or **Application Gateway**.
- **GCP:** Use **Cloud Load Balancing**.
- Combine Nginx with cloud LB for hybrid architectures or full cloud-native.
- Map Nginx Ingress Service to cloud LB with annotations in Kubernetes.

### D. Auto-Scaling with Health-Aware Backend Registration
- Implement auto-scaling groups for backend servers.
- Load balancer should dynamically register new healthy instances and remove unhealthy ones.
- Use health checks and instance lifecycle hooks.
- In Kubernetes, Horizontal Pod Autoscaler (HPA) adjusts backend pods automatically.

### E. Distributed Tracing/APM for Request Flow
- Integrate distributed tracing tools (OpenTelemetry, Jaeger, Zipkin, Datadog APM) to track requests across load balancer and backend.
- Use Nginx tracing modules or log exporters to propagate trace headers.
- Example:
  ```
  proxy_set_header traceparent $traceparent;
  ```
- Visualize request flow and latency in APM dashboards.

### F. Advanced Traffic Shaping & Rate Limiting
- Use Nginx modules (`limit_req`, `limit_conn`) for rate limiting and traffic shaping.
- Example:
  ```nginx
  limit_req zone=req_limit_per_ip burst=10 nodelay;
  limit_conn zone=addr 10;
  ```
- Combine with API gateway or cloud WAF for advanced controls.

### G. Global Load Balancing (GeoDNS, Anycast)
- Deploy GeoDNS to route users to nearest datacenter/load balancer.
- Use Anycast IP addresses for global failover and distribution.
- Integrate cloud DNS services (AWS Route 53, Azure Traffic Manager, Google Cloud DNS) for global routing.
- Example:
  - GeoDNS record for `app.example.com` points to regional Nginx LBs.
  - Each LB handles local traffic and failover.

---

## Step 8: Best Practices & Advanced Topics

- **Monitor Nginx and Backend Health:** Use Prometheus/Grafana exporters, check logs frequently.
- **Automate with IaC:** Use Ansible or Terraform to manage Nginx config and server deployment.
- **SSL/TLS Offloading:** Terminate HTTPS at the load balancer for security and performance.
- **Web Application Firewall (WAF):** Integrate ModSecurity or a cloud WAF with Nginx for enhanced security.
- **Containerize Load Balancer:** Deploy Nginx as a container or in Kubernetes with Ingress controllers for dynamic scaling.
- **CI/CD Integration:** Automate config updates and reloads via pipelines.
- **SELinux:** Always check and set required booleans for Nginx networking.

---

## References

- [Nginx Load Balancing Docs](https://nginx.org/en/docs/http/load_balancing.html)
- [Consul Service Discovery](https://www.consul.io/docs/discovery)
- [Kubernetes Ingress Controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
- [Argo Rollouts](https://argoproj.github.io/argo-rollouts/)
- [AWS Elastic Load Balancing](https://docs.aws.amazon.com/elasticloadbalancing/)
- [Azure Load Balancer](https://learn.microsoft.com/en-us/azure/load-balancer/)
- [GCP Load Balancing](https://cloud.google.com/load-balancing/)
- [OpenTelemetry](https://opentelemetry.io/)
- [Jaeger Tracing](https://www.jaegertracing.io/)
- [Nginx Rate Limiting](https://nginx.org/en/docs/http/ngx_http_limit_req_module.html)
- [GeoDNS with Route 53](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html)
- [Anycast networking](https://en.wikipedia.org/wiki/Anycast)
- [Prometheus Nginx Exporter](https://github.com/nginxinc/nginx-prometheus-exporter)
- [ModSecurity WAF](https://modsecurity.org/)

---

_Best Practice:_  
Automate deployment and configuration, monitor all layers, use modern load balancing algorithms, secure with SELinux/WAF/SSL, integrate dynamic cloud-native features (service discovery, scaling, tracing, traffic shaping, global routing), and containerize for scale and reliability.

---
